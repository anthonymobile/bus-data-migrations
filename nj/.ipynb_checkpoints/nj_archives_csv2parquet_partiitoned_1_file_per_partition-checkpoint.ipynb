{
 "cells": [
  {
   "cell_type": "raw",
   "id": "26e6deec",
   "metadata": {},
   "source": [
    "Load the FULL CSV with ALL rows and partition based on route id column 'rt' and write the separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mungingdata.com/python/writing-parquet-pandas-pyspark-koalas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401035d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"./data/in/buses_nj_thru_2022_03_28.csv\"\n",
    "outpath = \"./data/out/buses_nj_thru_2022_03_28_partitioned_csv_full\"\n",
    "outpath2 = \"./data/out/buses_nj_thru_2022_03_28_partitioned_csv_full_1_file_per_partition\"\n",
    "outpath3 = \"./data/out/buses_nj_thru_2022_03_28_partitioned_csv_full_1_file_per_partition_by_hour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb54150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/07 07:27:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/07 07:27:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/07 07:27:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType, LongType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .master(\"local\") \\\n",
    "  .appName(\"partioning_project\") \\\n",
    "  .config(\"spark.executor.cores\", 3) \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6ce103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # option 1, no schema\n",
    "# df = spark.read.csv(infile, header = True)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "606773f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # option 2, infer schema\n",
    "# df = spark.read.csv(infile, inferSchema = True, header = True)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90eac49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3, user-defined schema\n",
    "schema = StructType() \\\n",
    "      .add(\"pkey\",StringType(),True) \\\n",
    "      .add(\"lat\",DoubleType(),True) \\\n",
    "      .add(\"lon\",DoubleType(),True) \\\n",
    "      .add(\"cars\",StringType(),True) \\\n",
    "      .add(\"consist\",StringType(),True) \\\n",
    "      .add(\"d\",StringType(),True) \\\n",
    "      .add(\"dn\",StringType(),True) \\\n",
    "      .add(\"fs\",StringType(),True) \\\n",
    "      .add(\"id\",StringType(),True) \\\n",
    "      .add(\"m\",StringType(),True) \\\n",
    "      .add(\"op\",StringType(),True) \\\n",
    "      .add(\"pd\",StringType(),True) \\\n",
    "      .add(\"pdrtpifeedname\",StringType(),True) \\\n",
    "      .add(\"pid\",StringType(),True) \\\n",
    "      .add(\"rt\",StringType(),True) \\\n",
    "      .add(\"rtrtpifeedname\",StringType(),True) \\\n",
    "      .add(\"rtdd\",StringType(),True) \\\n",
    "      .add(\"rtpifeedname\",StringType(),True) \\\n",
    "      .add(\"run\",StringType(),True) \\\n",
    "      .add(\"wid1\",StringType(),True) \\\n",
    "      .add(\"wid2\",StringType(),True) \\\n",
    "      .add(\"timestamp\",StringType(),True)\n",
    "      # .add(\"timestamp\",TimestampType(),True)\n",
    "      # Spark doesn't recognize date64 types, and no way to create user-defined type\n",
    "      # possibly a solution here we can implement after the import by mapping the timestamp column? \n",
    "      # https://arrow.apache.org/docs/python/timestamps.html\n",
    "\n",
    "\n",
    "df = spark.read.csv(infile, header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3a3968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pkey: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- cars: string (nullable = true)\n",
      " |-- consist: string (nullable = true)\n",
      " |-- d: string (nullable = true)\n",
      " |-- dn: string (nullable = true)\n",
      " |-- fs: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- m: string (nullable = true)\n",
      " |-- op: string (nullable = true)\n",
      " |-- pd: string (nullable = true)\n",
      " |-- pdrtpifeedname: string (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- rt: string (nullable = true)\n",
      " |-- rtrtpifeedname: string (nullable = true)\n",
      " |-- rtdd: string (nullable = true)\n",
      " |-- rtpifeedname: string (nullable = true)\n",
      " |-- run: string (nullable = true)\n",
      " |-- wid1: string (nullable = true)\n",
      " |-- wid2: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49be5341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+----+-------+----------------+---+--------------------+-----+---+------+--------------------+--------------+----+---+--------------+----+------------+---+----+----+-------------------+\n",
      "|pkey|    lat|     lon|cars|consist|               d| dn|                  fs|   id|  m|    op|                  pd|pdrtpifeedname| pid| rt|rtrtpifeedname|rtdd|rtpifeedname|run|wid1|wid2|          timestamp|\n",
      "+----+-------+--------+----+-------+----------------+---+--------------------+-----+---+------+--------------------+--------------+----+---+--------------+----+------------+---+----+----+-------------------+\n",
      "|   1| 40.722|-74.2819|null|   null|      West Bound|WNW|           70 SUMMIT| 5819|  1|482580|Livingston/Florha...|            \\N|1666| 70|            \\N|  70|          \\N| 14|0014|0070|2021-04-05 23:16:58|\n",
      "|   2|40.7344|-74.1894|null|   null|      East Bound|ENE|25 SPRINGFIELD AV...| 5820|  1|547664|Doremus Avenue (N...|            \\N| 737| 25|            \\N|  25|          \\N| 21|0021|0025|2021-04-05 23:16:58|\n",
      "|   3|39.7289|-74.2655|null|   null|     North Bound|NNE|319 NEW YORK VIA ...|18112|  1|545583|     Newark/New York|            \\N|6890|319|            \\N| 319|          \\N|304|0304|0319|2021-04-05 23:16:58|\n",
      "|   4|40.7452|-74.2592|null|   null|South West Bound| SW|     92 SOUTH ORANGE| 5839|  1|   N/A|        South Orange|            \\N| 471| 92|            \\N|  92|          \\N|699|0699|0092|2021-04-05 23:16:58|\n",
      "|   5|38.9352|-74.9227|null|   null|North West Bound|NNW|552 CAPE MAY CRES...|18121|  1|547628|            Cape May|            \\N|1009|552|            \\N| 552|          \\N| 47|0047|0552|2021-04-05 23:16:58|\n",
      "+----+-------+--------+----+-------+----------------+---+--------------------+-----+---+------+--------------------+--------------+----+---+--------------+----+------------+---+----+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8632e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/spark/pyspark-to_timestamp-convert-string-to-timestamp-type/\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Timestamp String to DateType\n",
    "new_df = df.withColumn(\"timestamp\",to_timestamp(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998fcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pkey: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- cars: string (nullable = true)\n",
      " |-- consist: string (nullable = true)\n",
      " |-- d: string (nullable = true)\n",
      " |-- dn: string (nullable = true)\n",
      " |-- fs: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- m: string (nullable = true)\n",
      " |-- op: string (nullable = true)\n",
      " |-- pd: string (nullable = true)\n",
      " |-- pdrtpifeedname: string (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- rt: string (nullable = true)\n",
      " |-- rtrtpifeedname: string (nullable = true)\n",
      " |-- rtdd: string (nullable = true)\n",
      " |-- rtpifeedname: string (nullable = true)\n",
      " |-- run: string (nullable = true)\n",
      " |-- wid1: string (nullable = true)\n",
      " |-- wid2: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2cad8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+----+-------+----------------+---+--------------------+-----+---+------+--------------------+--------------+----+---+--------------+----+------------+---+----+----+-------------------+\n",
      "|pkey|    lat|     lon|cars|consist|               d| dn|                  fs|   id|  m|    op|                  pd|pdrtpifeedname| pid| rt|rtrtpifeedname|rtdd|rtpifeedname|run|wid1|wid2|          timestamp|\n",
      "+----+-------+--------+----+-------+----------------+---+--------------------+-----+---+------+--------------------+--------------+----+---+--------------+----+------------+---+----+----+-------------------+\n",
      "|   1| 40.722|-74.2819|null|   null|      West Bound|WNW|           70 SUMMIT| 5819|  1|482580|Livingston/Florha...|            \\N|1666| 70|            \\N|  70|          \\N| 14|0014|0070|2021-04-05 23:16:58|\n",
      "|   2|40.7344|-74.1894|null|   null|      East Bound|ENE|25 SPRINGFIELD AV...| 5820|  1|547664|Doremus Avenue (N...|            \\N| 737| 25|            \\N|  25|          \\N| 21|0021|0025|2021-04-05 23:16:58|\n",
      "|   3|39.7289|-74.2655|null|   null|     North Bound|NNE|319 NEW YORK VIA ...|18112|  1|545583|     Newark/New York|            \\N|6890|319|            \\N| 319|          \\N|304|0304|0319|2021-04-05 23:16:58|\n",
      "|   4|40.7452|-74.2592|null|   null|South West Bound| SW|     92 SOUTH ORANGE| 5839|  1|   N/A|        South Orange|            \\N| 471| 92|            \\N|  92|          \\N|699|0699|0092|2021-04-05 23:16:58|\n",
      "|   5|38.9352|-74.9227|null|   null|North West Bound|NNW|552 CAPE MAY CRES...|18121|  1|547628|            Cape May|            \\N|1009|552|            \\N| 552|          \\N| 47|0047|0552|2021-04-05 23:16:58|\n",
      "+----+-------+--------+----+-------+----------------+---+--------------------+-----+---+------+--------------------+--------------+----+---+--------------+----+------------+---+----+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eef226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.write \\\n",
    "#     .mode('overwrite')\\\n",
    "#     .partitionBy(\"rt\")\\\n",
    "#     .parquet(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb51716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write it with 1 file per partition (eliminate step 2)\n",
    "\n",
    "# new_df.repartition(\"rt\") \\\n",
    "#     .write \\\n",
    "#     .mode('overwrite')\\\n",
    "#     .partitionBy(\"rt\")\\\n",
    "#     .parquet(outpath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc2db5",
   "metadata": {},
   "source": [
    "# add a date hour columns and partition on that"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff042050",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/52527888/spark-partition-data-writing-by-timestamp/52528333#52528333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe5c0148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 07:27:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_df \\\n",
    "    .withColumn(\"year\", year(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "    .repartition(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "    .write \\\n",
    "    .mode('overwrite')\\\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "    .parquet(outpath3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b20f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
