{
 "cells": [
  {
   "cell_type": "raw",
   "id": "00229ff6",
   "metadata": {},
   "source": [
    "Workbench code for bundling a single day's individual grab parquet files up into a partitioned by route-hour folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath=\"/Users/anthonytownsend/Dropbox/Desktop/_code/notebooks/pyspark/data/in/njtransit_bus_singles_grabs_for_bundling\"\n",
    "outpath=\"/Users/anthonytownsend/Dropbox/Desktop/_code/notebooks/pyspark/data/out/njtransit_bus_singles_grabs_bundled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd08050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType, LongType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .master(\"local\") \\\n",
    "  .appName(\"partioning_project\") \\\n",
    "  .config(\"spark.executor.cores\", 3) \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1051dd",
   "metadata": {},
   "source": [
    "## probably wont need this stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2368467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # option 3, user-defined schema\n",
    "# schema = StructType() \\\n",
    "#       .add(\"pkey\",StringType(),True) \\\n",
    "#       .add(\"lat\",DoubleType(),True) \\\n",
    "#       .add(\"lon\",DoubleType(),True) \\\n",
    "#       .add(\"cars\",StringType(),True) \\\n",
    "#       .add(\"consist\",StringType(),True) \\\n",
    "#       .add(\"d\",StringType(),True) \\\n",
    "#       .add(\"dn\",StringType(),True) \\\n",
    "#       .add(\"fs\",StringType(),True) \\\n",
    "#       .add(\"id\",StringType(),True) \\\n",
    "#       .add(\"m\",StringType(),True) \\\n",
    "#       .add(\"op\",StringType(),True) \\\n",
    "#       .add(\"pd\",StringType(),True) \\\n",
    "#       .add(\"pdrtpifeedname\",StringType(),True) \\\n",
    "#       .add(\"pid\",StringType(),True) \\\n",
    "#       .add(\"rt\",StringType(),True) \\\n",
    "#       .add(\"rtrtpifeedname\",StringType(),True) \\\n",
    "#       .add(\"rtdd\",StringType(),True) \\\n",
    "#       .add(\"rtpifeedname\",StringType(),True) \\\n",
    "#       .add(\"run\",StringType(),True) \\\n",
    "#       .add(\"wid1\",StringType(),True) \\\n",
    "#       .add(\"wid2\",StringType(),True) \\\n",
    "#       .add(\"timestamp\",StringType(),True)\n",
    "#       # .add(\"timestamp\",TimestampType(),True)\n",
    "#       # Spark doesn't recognize date64 types, and no way to create user-defined type\n",
    "#       # possibly a solution here we can implement after the import by mapping the timestamp column? \n",
    "#       # https://arrow.apache.org/docs/python/timestamps.html\n",
    "\n",
    "\n",
    "# df = spark.read.csv(infile, header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://sparkbyexamples.com/spark/pyspark-to_timestamp-convert-string-to-timestamp-type/\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# #Timestamp String to DateType\n",
    "# new_df = df.withColumn(\"timestamp\",to_timestamp(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e77aa",
   "metadata": {},
   "source": [
    "### compute the date component columns on the fly as we write the partitioned files\n",
    "### 1 file per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the \n",
    "new_df \\\n",
    "    .withColumn(\"year\", year(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "    .repartition(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "    .write \\\n",
    "    .mode('overwrite')\\\n",
    "    .partitionBy(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "    .parquet(outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
