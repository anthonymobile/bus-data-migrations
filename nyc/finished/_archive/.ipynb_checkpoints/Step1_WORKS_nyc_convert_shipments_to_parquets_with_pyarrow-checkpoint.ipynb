{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c890d4",
   "metadata": {},
   "source": [
    "## NYCbuswatcher\n",
    "#### Step 1. Convert JSON shipments --> parquets using Pyarrow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb90606f",
   "metadata": {},
   "source": [
    "This notebook will iterate over all the shipments in \"shipment_folder\" and convert each JSON to a single parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aeb72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63f9160f",
   "metadata": {},
   "source": [
    "1. get list of shipment filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772119df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5832"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path=\"/Volumes/nice/bigdata/bus_depot/bus-data-migrations/nyc/data/in/nyc_shipments/2022/2/2\"\n",
    "\n",
    "shipment_list = []\n",
    "for root, _, filenames in os.walk(in_path):   \n",
    "    for filename in filenames:\n",
    "        shipment_list.append(os.path.join(root, filename))\n",
    "len(shipment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d072a06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/nice/bigdata/bus_depot/bus-data-migrations/nyc/data/in/nyc_shipments/2022/2/2/0/BX16/shipment_2022-2-2-0-BX16.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shipment_list[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f66425ea",
   "metadata": {},
   "source": [
    "2. make sure the data types are right\n",
    "\n",
    "next_stop_d_along_route    float64\n",
    "next_stop_d                float64\n",
    "\n",
    "these should both be strings\n",
    "timestamp should be ?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "657e7ddf",
   "metadata": {},
   "source": [
    "3. iterate over file list and write each Json as a single parquet file with pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ea2733",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/Volumes/nice/bigdata/bus_depot/bus-data-migrations/nyc/data/out/nyc_shipments_as_parquets/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v_/d7d6w88x3t716sh3x9pyjd600000gn/T/ipykernel_66633/3039529779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Volumes/nice/bigdata/bus_depot/bus-data-migrations/nyc/data/out/nyc_shipments_as_parquets/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bin/mambaforge/envs/bigdata/lib/python3.9/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/Volumes/nice/bigdata/bus_depot/bus-data-migrations/nyc/data/out/nyc_shipments_as_parquets/'"
     ]
    }
   ],
   "source": [
    "out_path=\"/Volumes/nice/bigdata/bus_depot/bus-data-migrations/nyc/data/out/nyc_shipments_as_parquets/\"\n",
    "os.makedirs(out_path, exist_ok=False)\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "for shipment in shipment_list:\n",
    "    with open(shipment) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        try:\n",
    "            df = pd.json_normalize(data, 'buses').convert_dtypes()\n",
    "            # print(f\"Success loading buses from {shipment}\")\n",
    "        except:\n",
    "            # print(f\"Error loading buses from {shipment}\")\n",
    "            pass\n",
    "\n",
    "        # not sure why this is so hard, but need to ensure every file has the same schema\n",
    "        \n",
    "        df[\"passenger_count\"] = pd.to_numeric(df[\"passenger_count\"],errors='coerce')\n",
    "        \n",
    "        if df.dtypes['passenger_count'] != \"float64\":\n",
    "            df['passenger_count'] = df['passenger_count'].astype(dtype = 'float64')\n",
    "            \n",
    "        if df.dtypes['bearing'] != \"float64\":\n",
    "            df['bearing'] = df['bearing'].astype(dtype = 'float64')\n",
    "            \n",
    "        if df.dtypes['next_stop_d_along_route'] != \"float64\":\n",
    "            df['next_stop_d_along_route'] = df['next_stop_d_along_route'].astype(dtype = 'float64')\n",
    "\n",
    "        if df.dtypes['next_stop_d'] != \"float64\":\n",
    "            df['next_stop_d'] = df['next_stop_d'].astype(dtype = 'float64')\n",
    "            \n",
    "        # write to disk\n",
    "        out_file=out_path+shipment.split('/')[-1].split('.')[0]+\".parquet\"\n",
    "        df.to_parquet(out_file, engine=\"fastparquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
